\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with`a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{optidef}
\usepackage{amsmath,amsthm,fullpage,amssymb,mathtools,enumitem}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{property}[theorem]{Property}
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\dev}{\mathrm{d}}
%\newcommand{\proof}{\textit{Proof.}}

\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\title{Optimization Methods for Nonparametric Convex Regression}
\author{Yunlang Zhu}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
    In this paper, we study the formulation of nonparametric convex regression as well as optimization methods for tackling this problem. Particularly, we discuss a general formulation of convex regression and then reformulate it to a computationally tractable quadratic programming problem. Furthermore, we present several recent papers that propose methods for solving convex regression with concerns on convergence guarantee, stability of solutions, and problem structure exploitation. 
\end{abstract}

\section{Introduction}
Convex regression is a problem aimed at finding a convex function to best fit a finite number of observations. This technique raises interest in numerous fields since convexity (or concavity) is desired when fitting observations in several scenarios. For example, in financial engineering, the option pricing function is restricted to be convex under the no-arbitrage condition \cite{Sahalia03}. In addition to the desire for convexity, convex regression can help us apply prior knowledge to find a reasonable fitting function when the available datasets are too small in size to contain enough information to be exploited. For instance, experiments for the press hardening process in mechanical engineering are so expensive that it is hard to collect large datasets; therefore, prior knowledge that the hardness grows concavely with increasing temperature \cite{Kurnatowski_2021} can be introduced to compensate for this data shortage and to help us find a physically meaningful function with limited data. 

To solve convex regression problems, there are usually two types of approaches, namely, parametric methods and nonparametric methods. In this paper, we will focus on optimization methods for nonparametric convex regression problems. Though we have witnessed advances in developing nonparametric methods, there are several challenges to be overcome, namely, the large-scale nature, the requirement for high accuracy, and the enormous complexity of the fitting functions obtained. In the remainder part of the paper, we will discuss the mathematical background of this topic in Section \ref{s2}, as well as several recent works done in this field in Section \ref{s3}.

\section{Mathematical Background} \label{s2}
To begin with, let us first recall the definitions of convex sets and convex functions. A set $C$ is convex if for any two points $x,y \in C$, and for any $0 \leq \theta \leq 1$, we have
\begin{equation}
    \theta x + (1-\theta) y \in C.
\end{equation}
Moreover, if the domain of function $f: \mathbb{R}^d \rightarrow \mathbb{R}$ is a convex set, and for any $x,y$ in the domain and any $0 \leq \theta \leq 1$, we have
\begin{equation} \label{conv1}
    f(\theta x + (1-\theta) y) \leq \theta f(x) + (1-\theta) f(y),
\end{equation}
then $f$ is a convex function. In addition, if $f$ is differentiable, then it is convex if and only if
\begin{equation}
    f(x)+\nabla f(x)^T (y-x) \leq f(y) \quad \forall x,y.
\end{equation}

Now suppose we have $n$ observations $\{(x_i,y_i)\} \in \mathbb{R}^d \times \mathbb{R}$ from our data, where $x_i \in \mathbb{R}^d$ is a vector for independent variables (features), and $y_i$ is the corresponding response variable for $x_i$. Generally, we estimate the function of interest with a least squares estimator over all convex functions $\psi: \mathbb{R}^d \rightarrow \mathbb{R}$ satisfying
\begin{equation} \label{lr1}
    y_i = \psi(x_i) + \epsilon
\end{equation}
where $\epsilon$ is some noise with its expectation given the set of independent variables $X=(x_1,\dots,x_n)^T \in \mathbb{R}^{n \times d}$ being zero, that is, $\E[\epsilon|X]=0$. Notice that we can perform concave regression as well, since the requirement that $f$ is convex is equivalent to constraining $-f$ to be concave.  The least squares estimator $\hat{\psi}$ is defined as
\begin{equation} \label{lr2}
    \hat{\psi} \in \argmin_{\psi \in C} \sum_{i=1}^n (y_i-\psi(x_i))^2
\end{equation}
where $C:=\{\psi:\mathbb{R}^d \rightarrow \mathbb{R} \;|\text{ $\psi$ is a convex function} \}$. By deriving the least squares estimator, we are minimizing the squared error, also known as the $l_2$ loss function:
\begin{equation} \label{loss}
    \text{Loss}_{l_2}=\sum_{i=1}^n (y_i-\psi(x_i))^2.
\end{equation}
Similarly, one can also perform optimization over the absolute error, also known as the $l_1$ loss function:
\begin{equation} \label{loss}
    \text{Loss}_{l_1}=\sum_{i=1}^n |y_i-\psi(x_i)|.
\end{equation}

The optimization problem over functions in \eqref{lr2} appears to be infinitely dimensional and therefore intractable. However, it can be reformulated to an equivalent Quadratic Programming (QP) problem with $\theta_i = \hat{\psi}(x_i)$ for $i=1,\dots,n$; $\theta=(\theta_1,\dots,\theta_n)^T$, and $Y=(y_1,\dots,y_n)^T$ (this formulation is originally proposed by \cite{Seijo11}, which will not be discussed in this paper). The equivalent QP problem is
\begin{align}  \label{lr3}
    \min_{\xi_1,\dots,\xi_n;\theta} \quad &\frac{1}{2}||Y-\theta||_2^2 \\
    \label{conv}
    \textrm{s.t.} \quad\quad  &\theta_i+\xi_i^T(x_j-x_i) \leq \theta_j \quad \forall i,j, \\
    &\theta \in \mathbb{R}^n, \\
    &\xi_i \in \mathbb{R}^d \quad \forall i.
\end{align}
Notice that $\xi_i$ in the formulation above \eqref{conv} is the subgradient (or subderivative) at $x_i$. Subgradient is used here instead of derivative since the function might not be differentiable, e.g., the function can be piecewise linear. Mathematically, a vector $g \in \mathbb{R}^d$ is a subgradient of function $\psi$ at a point $x_i$ if
\begin{equation} \label{subgradient}
    \psi(x_i)+g^T(x_j-x_i) \leq \psi(x_j) \quad \forall j.
\end{equation}
Furthermore, the constraint \eqref{conv} is used to ensure convexity.

\section{Literature Review} \label{s3}
In the literature, there are various methods for solving the convex regression problem \eqref{lr3} and its extensions. In this section, we review the relevant literature. Aiming at solving Problem \eqref{lr3}, Mazumder et al. \cite{Mazumder18} propose an algorithmic framework based on augmented Lagrangian method and alternating direction method of multipliers (ADMM). First, they take the augmented Lagrangian of the original Problem \eqref{lr3}. Then they employ a 3-block version of ADMM for the augmented Lagrangian (we refer our readers to \cite{Mazumder18} for full details). The cost for performing this algorithm is $O(\max\{n^2d,nd^3\})$ with an additional $O(n^2d^2+nd^3)$ for the computation of matrix inverses for updating $\xi_i$. This algorithm leaves some questions, and two major ones are about convergence properties and method extension respectively. As a 3-block ADMM, the method has no convergence guarantee, since we can only secure convergence for 2-block ADMM instances. Though the paper proposes another convergence-guaranteed algorithm based on augmented Lagrangian method of multipliers, its computational cost is so high that it is needed to be initialized with a solution obtained from ADMM. On the other hand, the method cannot be easily extended for least absolute deviation convex regression, where the loss function is in $\l_1$ form instead of $l_2$. In addition, the feasible set of Problem \eqref{lr3} can be unbounded. As a result, this may lead to instability, since different values of subgradients $\xi_i$ can achieve the same cost function value. 

In order to eliminate instability, Bertsimas and Mundru \cite{Bertsimas20} add a regularization term to the objective function in Problem \eqref{lr3}. The added regularization term leads to a strongly convex function, ensuring that $\xi_i$ can only take some certain value for optimum. To extend this problem, \cite{Bertsimas20} also proposes a formulation for $l_1$ convex regression. In order to guarantee convergence for solving the original Problem \eqref{lr3}, which is not completely handled by \cite{Mazumder18}, \cite{Bertsimas20} presents a cutting-plane algorithm; this algorithm converges to an optimal solution within a finite number of iterations. The proposed method starts with an initial reduced master problem with $(n-1)$ constraints (the original problem has $n(n-1)$ constraints), which is assumed to be solved by Gurobi. Next, the solution found is verified with the entire set of constraints (we refer our readers to \cite{Bertsimas20} for full details). Though \cite{Bertsimas20} presents an approach to ensure convergence, this work applies an off-the-shelf solver, instead of developing a complete algorithm that exploits the special structure of this problem. Their work also discusses sparse convex regression, where the union of supports of $\xi_i$ in each point $x_i$ is a set whose cardinality is bounded by $k$, but that is out of our scope here.

To guarantee convergence while utilizing the structure of Problem \eqref{lr3}, Lin et al. \cite{Lin22} propose a proximal augmented Lagrangian method (proxALM) with a semismooth Newton method for solving the proxALM subproblems. This paper also assumes additional shape constraints on subgradients $\xi_i$; the simply convex constraints are extended to more shape constraints, including monotone constraints, box constraints, and Lipschitz constraints. To solve the resulting problem, \cite{Lin22} develops a proxALM method. It is proximal since when updating $\theta$ and $\xi$, the algorithm calculates an approximate solution satisfying some stopping criterion. Particularly, updating $\theta,\xi$ is the most computationally intensive step. To deal with it, \cite{Lin22} designs a semismooth Newton method for this subproblem (we refer our readers to \cite{Lin22} for full details). 

\bibliographystyle{plain}
\bibliography{literature_review}
\end{document}

